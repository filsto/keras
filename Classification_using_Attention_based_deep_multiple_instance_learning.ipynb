{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification using Attention based deep multiple instance learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5BJSTkM4jDa+J1ibygGgz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gorphe/keras/blob/master/Classification_using_Attention_based_deep_multiple_instance_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Mohamad Jaber\n",
        "\n",
        "Date created: 2021/08/16\n",
        "\n",
        "Last modified: 2021/11/25\n",
        "\n",
        "Description: MIL approach to classify bags of instances and get their individual instance score.\n",
        "\n"
      ],
      "metadata": {
        "id": "XUJ_GYIC8kx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Multiple Instance Learning (MIL)?**\n",
        "\n",
        "Usually, with supervised learning algorithms, the learner receives labels for a set of instances. In the case of MIL, the learner receives labels for a set of bags, each of which contains a set of instances. The bag is labeled positive if it contains at least one positive instance, and negative if it does not contain any.\n",
        "\n",
        "**Motivation**\n",
        "\n",
        "It is often assumed in image classification tasks that each image clearly represents a class label. In medical imaging (e.g. computational pathology, etc.) an entire image is represented by a single class label (cancerous/non-cancerous) or a region of interest could be given. However, one will be interested in knowing which patterns in the image is actually causing it to belong to that class. In this context, the image(s) will be divided and the subimages will form the bag of instances.\n",
        "\n",
        "Therefore, the goals are to:\n",
        "\n",
        "Learn a model to predict a class label for a bag of instances.\n",
        "\n",
        "Find out which instances within the bag caused a position class label prediction.\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "The following steps describe how the model works:\n",
        "\n",
        "The feature extractor layers extract feature embeddings.\n",
        "\n",
        "The embeddings are fed into the MIL attention layer to get the attention scores. The layer is designed as permutation-invariant.\n",
        "\n",
        "Input features and their corresponding attention scores are multiplied together.\n",
        "\n",
        "The resulting output is passed to a softmax function for classification.\n",
        "\n",
        "**References**\n",
        "\n",
        "Attention-based Deep Multiple Instance Learning.\n",
        "Some of the attention operator code implementation was inspired from https://github.com/utayao/Atten_Deep_MIL.\n",
        "Imbalanced data tutorial by TensorFlow."
      ],
      "metadata": {
        "id": "baE3gCrJ8tpA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu0F3YJ57o3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset"
      ],
      "metadata": {
        "id": "fKyrm3Cc-smK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "POSITIVE_CLASS = 1\n",
        "BAG_COUNT = 1000\n",
        "VAL_BAG_COUNT = 300\n",
        "BAG_SIZE = 3\n",
        "PLOT_SIZE = 3\n",
        "ENSEMBLE_AVG_COUNT = 1"
      ],
      "metadata": {
        "id": "sdnv1P32-uCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bags(input_data, input_labels, positive_class, bag_count, instance_count):\n",
        "\n",
        "    # Set up bags.\n",
        "    bags = []\n",
        "    bag_labels = []\n",
        "\n",
        "    # Normalize input data.\n",
        "    input_data = np.divide(input_data, 255.0)\n",
        "\n",
        "    # Count positive samples.\n",
        "    count = 0\n",
        "\n",
        "    for _ in range(bag_count):\n",
        "\n",
        "        # Pick a fixed size random subset of samples.\n",
        "        index = np.random.choice(input_data.shape[0], instance_count, replace=False)\n",
        "        instances_data = input_data[index]\n",
        "        instances_labels = input_labels[index]\n",
        "\n",
        "        # By default, all bags are labeled as 0.\n",
        "        bag_label = 0\n",
        "\n",
        "        # Check if there is at least a positive class in the bag.\n",
        "        if positive_class in instances_labels:\n",
        "\n",
        "            # Positive bag will be labeled as 1.\n",
        "            bag_label = 1\n",
        "            count += 1\n",
        "\n",
        "        bags.append(instances_data)\n",
        "        bag_labels.append(np.array([bag_label]))\n",
        "\n",
        "    print(f\"Positive bags: {count}\")\n",
        "    print(f\"Negative bags: {bag_count - count}\")\n",
        "\n",
        "    return (list(np.swapaxes(bags, 0, 1)), np.array(bag_labels))\n",
        "\n",
        "\n",
        "# Load the MNIST dataset.\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Create training data.\n",
        "train_data, train_labels = create_bags(\n",
        "    x_train, y_train, POSITIVE_CLASS, BAG_COUNT, BAG_SIZE\n",
        ")\n",
        "\n",
        "# Create validation data.\n",
        "val_data, val_labels = create_bags(\n",
        "    x_val, y_val, POSITIVE_CLASS, VAL_BAG_COUNT, BAG_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "bOK3XNgM_aI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model"
      ],
      "metadata": {
        "id": "5CQKQK2SBRY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MILAttentionLayer(layers.Layer):\n",
        "    \"\"\"Implementation of the attention-based Deep MIL layer.\n",
        "\n",
        "    Args:\n",
        "      weight_params_dim: Positive Integer. Dimension of the weight matrix.\n",
        "      kernel_initializer: Initializer for the `kernel` matrix.\n",
        "      kernel_regularizer: Regularizer function applied to the `kernel` matrix.\n",
        "      use_gated: Boolean, whether or not to use the gated mechanism.\n",
        "\n",
        "    Returns:\n",
        "      List of 2D tensors with BAG_SIZE length.\n",
        "      The tensors are the attention scores after softmax with shape `(batch_size, 1)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        weight_params_dim,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        kernel_regularizer=None,\n",
        "        use_gated=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.weight_params_dim = weight_params_dim\n",
        "        self.use_gated = use_gated\n",
        "\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "\n",
        "        self.v_init = self.kernel_initializer\n",
        "        self.w_init = self.kernel_initializer\n",
        "        self.u_init = self.kernel_initializer\n",
        "\n",
        "        self.v_regularizer = self.kernel_regularizer\n",
        "        self.w_regularizer = self.kernel_regularizer\n",
        "        self.u_regularizer = self.kernel_regularizer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        # Input shape.\n",
        "        # List of 2D tensors with shape: (batch_size, input_dim).\n",
        "        input_dim = input_shape[0][1]\n",
        "\n",
        "        self.v_weight_params = self.add_weight(\n",
        "            shape=(input_dim, self.weight_params_dim),\n",
        "            initializer=self.v_init,\n",
        "            name=\"v\",\n",
        "            regularizer=self.v_regularizer,\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        self.w_weight_params = self.add_weight(\n",
        "            shape=(self.weight_params_dim, 1),\n",
        "            initializer=self.w_init,\n",
        "            name=\"w\",\n",
        "            regularizer=self.w_regularizer,\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        if self.use_gated:\n",
        "            self.u_weight_params = self.add_weight(\n",
        "                shape=(input_dim, self.weight_params_dim),\n",
        "                initializer=self.u_init,\n",
        "                name=\"u\",\n",
        "                regularizer=self.u_regularizer,\n",
        "                trainable=True,\n",
        "            )\n",
        "        else:\n",
        "            self.u_weight_params = None\n",
        "\n",
        "        self.input_built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        # Assigning variables from the number of inputs.\n",
        "        instances = [self.compute_attention_scores(instance) for instance in inputs]\n",
        "\n",
        "        # Apply softmax over instances such that the output summation is equal to 1.\n",
        "        alpha = tf.math.softmax(instances, axis=0)\n",
        "\n",
        "        return [alpha[i] for i in range(alpha.shape[0])]\n",
        "\n",
        "    def compute_attention_scores(self, instance):\n",
        "\n",
        "        # Reserve in-case \"gated mechanism\" used.\n",
        "        original_instance = instance\n",
        "\n",
        "        # tanh(v*h_k^T)\n",
        "        instance = tf.math.tanh(tf.tensordot(instance, self.v_weight_params, axes=1))\n",
        "\n",
        "        # for learning non-linear relations efficiently.\n",
        "        if self.use_gated:\n",
        "\n",
        "            instance = instance * tf.math.sigmoid(\n",
        "                tf.tensordot(original_instance, self.u_weight_params, axes=1)\n",
        "            )\n",
        "\n",
        "        # w^T*(tanh(v*h_k^T)) / w^T*(tanh(v*h_k^T)*sigmoid(u*h_k^T))\n",
        "        return tf.tensordot(instance, self.w_weight_params, axes=1)"
      ],
      "metadata": {
        "id": "fuqtJuISBbh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizer tool"
      ],
      "metadata": {
        "id": "MUrjryqCBcOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(data, labels, bag_class, predictions=None, attention_weights=None):\n",
        "\n",
        "    \"\"\"\"Utility for plotting bags and attention weights.\n",
        "\n",
        "    Args:\n",
        "      data: Input data that contains the bags of instances.\n",
        "      labels: The associated bag labels of the input data.\n",
        "      bag_class: String name of the desired bag class.\n",
        "        The options are: \"positive\" or \"negative\".\n",
        "      predictions: Class labels model predictions.\n",
        "      If you don't specify anything, ground truth labels will be used.\n",
        "      attention_weights: Attention weights for each instance within the input data.\n",
        "      If you don't specify anything, the values won't be displayed.\n",
        "    \"\"\"\n",
        "\n",
        "    labels = np.array(labels).reshape(-1)\n",
        "\n",
        "    if bag_class == \"positive\":\n",
        "        if predictions is not None:\n",
        "            labels = np.where(predictions.argmax(1) == 1)[0]\n",
        "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
        "\n",
        "        else:\n",
        "            labels = np.where(labels == 1)[0]\n",
        "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
        "\n",
        "    elif bag_class == \"negative\":\n",
        "        if predictions is not None:\n",
        "            labels = np.where(predictions.argmax(1) == 0)[0]\n",
        "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
        "        else:\n",
        "            labels = np.where(labels == 0)[0]\n",
        "            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n",
        "\n",
        "    else:\n",
        "        print(f\"There is no class {bag_class}\")\n",
        "        return\n",
        "\n",
        "    print(f\"The bag class label is {bag_class}\")\n",
        "    for i in range(PLOT_SIZE):\n",
        "        figure = plt.figure(figsize=(8, 8))\n",
        "        print(f\"Bag number: {labels[i]}\")\n",
        "        for j in range(BAG_SIZE):\n",
        "            image = bags[j][i]\n",
        "            figure.add_subplot(1, BAG_SIZE, j + 1)\n",
        "            plt.grid(False)\n",
        "            if attention_weights is not None:\n",
        "                plt.title(np.around(attention_weights[labels[i]][j], 2))\n",
        "            plt.imshow(image)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Plot some of validation data bags per class.\n",
        "plot(val_data, val_labels, \"positive\")\n",
        "plot(val_data, val_labels, \"negative\")"
      ],
      "metadata": {
        "id": "YDD7nwGCBhnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model"
      ],
      "metadata": {
        "id": "r9OyRdOJBjku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(instance_shape):\n",
        "\n",
        "    # Extract features from inputs.\n",
        "    inputs, embeddings = [], []\n",
        "    shared_dense_layer_1 = layers.Dense(128, activation=\"relu\")\n",
        "    shared_dense_layer_2 = layers.Dense(64, activation=\"relu\")\n",
        "    for _ in range(BAG_SIZE):\n",
        "        inp = layers.Input(instance_shape)\n",
        "        flatten = layers.Flatten()(inp)\n",
        "        dense_1 = shared_dense_layer_1(flatten)\n",
        "        dense_2 = shared_dense_layer_2(dense_1)\n",
        "        inputs.append(inp)\n",
        "        embeddings.append(dense_2)\n",
        "\n",
        "    # Invoke the attention layer.\n",
        "    alpha = MILAttentionLayer(\n",
        "        weight_params_dim=256,\n",
        "        kernel_regularizer=keras.regularizers.l2(0.01),\n",
        "        use_gated=True,\n",
        "        name=\"alpha\",\n",
        "    )(embeddings)\n",
        "\n",
        "    # Multiply attention weights with the input layers.\n",
        "    multiply_layers = [\n",
        "        layers.multiply([alpha[i], embeddings[i]]) for i in range(len(alpha))\n",
        "    ]\n",
        "\n",
        "    # Concatenate layers.\n",
        "    concat = layers.concatenate(multiply_layers, axis=1)\n",
        "\n",
        "    # Classification output node.\n",
        "    output = layers.Dense(2, activation=\"softmax\")(concat)\n",
        "\n",
        "    return keras.Model(inputs, output)"
      ],
      "metadata": {
        "id": "yt-7anIEBmvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class weights"
      ],
      "metadata": {
        "id": "ei3ZypkYBoVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_weights(labels):\n",
        "\n",
        "    # Count number of postive and negative bags.\n",
        "    negative_count = len(np.where(labels == 0)[0])\n",
        "    positive_count = len(np.where(labels == 1)[0])\n",
        "    total_count = negative_count + positive_count\n",
        "\n",
        "    # Build class weight dictionary.\n",
        "    return {\n",
        "        0: (1 / negative_count) * (total_count / 2),\n",
        "        1: (1 / positive_count) * (total_count / 2),\n",
        "    }"
      ],
      "metadata": {
        "id": "dF3zVHZCBryv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and train model"
      ],
      "metadata": {
        "id": "RrUjk842Bss3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_data, train_labels, val_data, val_labels, model):\n",
        "\n",
        "    # Train model.\n",
        "    # Prepare callbacks.\n",
        "    # Path where to save best weights.\n",
        "\n",
        "    # Take the file name from the wrapper.\n",
        "    file_path = \"/tmp/best_model_weights.h5\"\n",
        "\n",
        "    # Initialize model checkpoint callback.\n",
        "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        file_path,\n",
        "        monitor=\"val_loss\",\n",
        "        verbose=0,\n",
        "        mode=\"min\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    # Initialize early stopping callback.\n",
        "    # The model performance is monitored across the validation data and stops training\n",
        "    # when the generalization error cease to decrease.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, mode=\"min\"\n",
        "    )\n",
        "\n",
        "    # Compile model.\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    # Fit model.\n",
        "    model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_data=(val_data, val_labels),\n",
        "        epochs=20,\n",
        "        class_weight=compute_class_weights(train_labels),\n",
        "        batch_size=1,\n",
        "        callbacks=[early_stopping, model_checkpoint],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    # Load best weights.\n",
        "    model.load_weights(file_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Building model(s).\n",
        "instance_shape = train_data[0][0].shape\n",
        "models = [create_model(instance_shape) for _ in range(ENSEMBLE_AVG_COUNT)]\n",
        "\n",
        "# Show single model architecture.\n",
        "print(models[0].summary())\n",
        "\n",
        "# Training model(s).\n",
        "trained_models = [\n",
        "    train(train_data, train_labels, val_data, val_labels, model)\n",
        "    for model in tqdm(models)\n",
        "]"
      ],
      "metadata": {
        "id": "3kwVva79Bt90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model evaluation"
      ],
      "metadata": {
        "id": "6VWLIPafByNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(data, labels, trained_models):\n",
        "\n",
        "    # Collect info per model.\n",
        "    models_predictions = []\n",
        "    models_attention_weights = []\n",
        "    models_losses = []\n",
        "    models_accuracies = []\n",
        "\n",
        "    for model in trained_models:\n",
        "\n",
        "        # Predict output classes on data.\n",
        "        predictions = model.predict(data)\n",
        "        models_predictions.append(predictions)\n",
        "\n",
        "        # Create intermediate model to get MIL attention layer weights.\n",
        "        intermediate_model = keras.Model(model.input, model.get_layer(\"alpha\").output)\n",
        "\n",
        "        # Predict MIL attention layer weights.\n",
        "        intermediate_predictions = intermediate_model.predict(data)\n",
        "\n",
        "        attention_weights = np.squeeze(np.swapaxes(intermediate_predictions, 1, 0))\n",
        "        models_attention_weights.append(attention_weights)\n",
        "\n",
        "        loss, accuracy = model.evaluate(data, labels, verbose=0)\n",
        "        models_losses.append(loss)\n",
        "        models_accuracies.append(accuracy)\n",
        "\n",
        "    print(\n",
        "        f\"The average loss and accuracy are {np.sum(models_losses, axis=0) / ENSEMBLE_AVG_COUNT:.2f}\"\n",
        "        f\" and {100 * np.sum(models_accuracies, axis=0) / ENSEMBLE_AVG_COUNT:.2f} % resp.\"\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        np.sum(models_predictions, axis=0) / ENSEMBLE_AVG_COUNT,\n",
        "        np.sum(models_attention_weights, axis=0) / ENSEMBLE_AVG_COUNT,\n",
        "    )\n",
        "\n",
        "\n",
        "# Evaluate and predict classes and attention scores on validation data.\n",
        "class_predictions, attention_params = predict(val_data, val_labels, trained_models)\n",
        "\n",
        "# Plot some results from our validation data.\n",
        "plot(\n",
        "    val_data,\n",
        "    val_labels,\n",
        "    \"positive\",\n",
        "    predictions=class_predictions,\n",
        "    attention_weights=attention_params,\n",
        ")\n",
        "plot(\n",
        "    val_data,\n",
        "    val_labels,\n",
        "    \"negative\",\n",
        "    predictions=class_predictions,\n",
        "    attention_weights=attention_params,\n",
        ")"
      ],
      "metadata": {
        "id": "f1NnKKC9BzmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}