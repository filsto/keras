{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semisupervision AdaMatch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+Y7xudk/026435ijvhfA2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filsto/keras/blob/master/Semisupervision_AdaMatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semi supervision and domain adaptation with AdaMatch**\n",
        "\n",
        "Author: Sayak Paul\n",
        "\n",
        "Date created: 2021/06/19\n",
        "\n",
        "Last modified: 2021/06/19\n",
        "\n",
        "Description: Unifying semi-supervised learning and unsupervised domain adaptation with AdaMatch."
      ],
      "metadata": {
        "id": "IN5wt_TsYegw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrroNsE7YQvE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from official.vision.image_classification.augment import RandAugment\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data"
      ],
      "metadata": {
        "id": "eo8k2QaZafBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST\n",
        "(\n",
        "   (mnist_x_train, mnist_y_train),\n",
        "   (mnist_x_test, mnist_y_test), \n",
        ") = keras.datasets.mnist.load_data()\n",
        "\n",
        "#Add a channel dimension\n",
        "mnist_x_train = tf.expand_dims(mnist_x_train, -1)\n",
        "mnist_x_test = tf.expand_dims(mnist_x_test, -1)\n",
        "\n",
        "#Convert the labels to one-hot encoded vectors\n",
        "mnist_y_train = tf.one_hot(mnist_y_train, 10).numpy()\n",
        "\n",
        "#SVHN\n",
        "svhn_train, svhn_test = tfds.load( \"svhn_cropped\", split=[\"train\", \"test\"], as_supervised=True)\n"
      ],
      "metadata": {
        "id": "31O4F1_FagkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define constants and hyperparameters"
      ],
      "metadata": {
        "id": "Irs0EB3JcSyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RESIZE_TO = 32\n",
        "\n",
        "SOURCE_BATCH_SIZE = 64\n",
        "TARGET_BATCH_SIZE = 3 * SOURCE_BATCH_SIZE\n",
        "EPOCHS = 10\n",
        "STEPS_PER_EPOCH = len(mnist_x_train) // SOURCE_BATCH_SIZE\n",
        "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCHS\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "LEARNING_RATE = 0.03\n",
        "\n",
        "WEIGHT_DECAY = 0.0005\n",
        "INIT = \"he_normal\"\n",
        "DEPTH = 28\n",
        "WIDTH_MULT = 2\n"
      ],
      "metadata": {
        "id": "64TbSSvFcVUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation utilities\n",
        "\n",
        "a standard element of SSL algorithms is to feed weakly and strongly augmented versions of the same images to the learning model to make its predictions consistent.\n",
        "\n",
        "for strong augmentation, RandAugment is a standard choice. for weak augmentation, we will use horizontal flipping and random cropping. "
      ],
      "metadata": {
        "id": "5ZuR7CEVdPYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize RandAugment object with 2 layers of augmentation transforms and strength of 5\n",
        "augmenter = RandAugment(num_layers=2, magnitude=5)\n",
        "\n",
        "def weak_augment(image, source=True):\n",
        "  if image.dtype != tf.float32:\n",
        "    image = tf.cast(image, tf.float32)\n",
        "  \n",
        "  #MNIST images are grayscale, this why we first convert them to RGB images\n",
        "  if source:\n",
        "    image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n",
        "    image = tf.tile(image, [1,1,3])\n",
        "  image = tf.image.random_flip_left.right(image)\n",
        "  image = tf.image.random_crop(image, (RESIZE_TO, RESIZE_TO,3))\n",
        "  return image\n",
        "\n",
        "def strong_augment(image, source=True):\n",
        "  if image.dtype != tf.float32:\n",
        "    image = tf.cast(image, tf.float32)\n",
        "\n",
        "  if source:\n",
        "    image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n",
        "    image = tf.tile(image, [1,1,3])\n",
        "\n",
        "  image = augmenter.distort(image)\n",
        "  return image"
      ],
      "metadata": {
        "id": "ntZ_HsHdu8YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading utilities"
      ],
      "metadata": {
        "id": "eJbB_ZT1zwgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_individual_ds(ds, aug_func, source=True):\n",
        "  if source:\n",
        "    batch_size = SOURCE_BATCH_SIZE\n",
        "  else:\n",
        "    #during trainin 3x more target unlabeled samples are shown\n",
        "    # to the model in AdaMatch(section 3.2 of the paper)\n",
        "    batch_size = TARGET_BATCH_SIZE\n",
        "  ds = ds.shuffle(batch_size*10, seed=42)\n",
        "\n",
        "  if source:\n",
        "    ds = ds.map(lambda x,y: (aug_func(x),y), num_parallel_calls=AUTO)\n",
        "  else:\n",
        "    ds = ds.map(lambda x,y: (aug_func(x, False),y), num_parallel_calls=AUTO)\n",
        "  ds = ds.batch(batch_size).prefetch(AUTO)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "98xVWpWczy51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_ds = tf.data.Dataset.from_tensor_slices((mnist_x_train, mnist_y_train))\n",
        "source_ds_w = create_individual_ds(source_ds, weak_augment)\n",
        "source_ds_s = create_individual_ds(source_ds, strong_augment)\n",
        "final_source_ds = tf.data.Dataset.zip((source_ds_w, source_ds_s))\n",
        "\n",
        "target_ds_w = create_individual_ds(svhn_train, weak_augment, source=False)\n",
        "target_ds_s = create_individual_ds(svhn_train, strong_augment, source=False)\n",
        "final_target_ds = tf.data.Dataset.zip((target_ds_w, target_ds_s))"
      ],
      "metadata": {
        "id": "mbagq5Xo0qRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss computation utilities"
      ],
      "metadata": {
        "id": "DxZgfE9I0uwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss_source(source_labels, logits_source_w, logits_source_s):\n",
        "    loss_func = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    # First compute the losses between original source labels and\n",
        "    # predictions made on the weakly and strongly augmented versions\n",
        "    # of the same images.\n",
        "    w_loss = loss_func(source_labels, logits_source_w)\n",
        "    s_loss = loss_func(source_labels, logits_source_s)\n",
        "    return w_loss + s_loss\n",
        "\n",
        "\n",
        "def compute_loss_target(target_pseudo_labels_w, logits_target_s, mask):\n",
        "    loss_func = keras.losses.CategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "    target_pseudo_labels_w = tf.stop_gradient(target_pseudo_labels_w)\n",
        "    # For calculating loss for the target samples, we treat the pseudo labels\n",
        "    # as the ground-truth. These are not considered during backpropagation\n",
        "    # which is a standard SSL practice.\n",
        "    target_loss = loss_func(target_pseudo_labels_w, logits_target_s)\n",
        "\n",
        "    # More on `mask` later.\n",
        "    mask = tf.cast(mask, target_loss.dtype)\n",
        "    target_loss *= mask\n",
        "    return tf.reduce_mean(target_loss, 0)"
      ],
      "metadata": {
        "id": "YFEAsVHh0xDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subclassed model for AdaMatch training"
      ],
      "metadata": {
        "id": "nVJQ4Ctk1DLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaMatch(keras.Model):\n",
        "    def __init__(self, model, total_steps, tau=0.9):\n",
        "        super(AdaMatch, self).__init__()\n",
        "        self.model = model\n",
        "        self.tau = tau  # Denotes the confidence threshold\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.total_steps = total_steps\n",
        "        self.current_step = tf.Variable(0, dtype=\"int64\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "    # This is a warmup schedule to update the weight of the\n",
        "    # loss contributed by the target unlabeled samples. More\n",
        "    # on this in the text.\n",
        "    def compute_mu(self):\n",
        "        pi = tf.constant(np.pi, dtype=\"float32\")\n",
        "        step = tf.cast(self.current_step, dtype=\"float32\")\n",
        "        return 0.5 - tf.cos(tf.math.minimum(pi, (2 * pi * step) / self.total_steps)) / 2\n",
        "\n",
        "    def train_step(self, data):\n",
        "        ## Unpack and organize the data ##\n",
        "        source_ds, target_ds = data\n",
        "        (source_w, source_labels), (source_s, _) = source_ds\n",
        "        (\n",
        "            (target_w, _),\n",
        "            (target_s, _),\n",
        "        ) = target_ds  # Notice that we are NOT using any labels here.\n",
        "\n",
        "        combined_images = tf.concat([source_w, source_s, target_w, target_s], 0)\n",
        "        combined_source = tf.concat([source_w, source_s], 0)\n",
        "\n",
        "        total_source = tf.shape(combined_source)[0]\n",
        "        total_target = tf.shape(tf.concat([target_w, target_s], 0))[0]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            ## Forward passes ##\n",
        "            combined_logits = self.model(combined_images, training=True)\n",
        "            z_d_prime_source = self.model(\n",
        "                combined_source, training=False\n",
        "            )  # No BatchNorm update.\n",
        "            z_prime_source = combined_logits[:total_source]\n",
        "\n",
        "            ## 1. Random logit interpolation for the source images ##\n",
        "            lambd = tf.random.uniform((total_source, 10), 0, 1)\n",
        "            final_source_logits = (lambd * z_prime_source) + (\n",
        "                (1 - lambd) * z_d_prime_source\n",
        "            )\n",
        "\n",
        "            ## 2. Distribution alignment (only consider weakly augmented images) ##\n",
        "            # Compute softmax for logits of the WEAKLY augmented SOURCE images.\n",
        "            y_hat_source_w = tf.nn.softmax(final_source_logits[: tf.shape(source_w)[0]])\n",
        "\n",
        "            # Extract logits for the WEAKLY augmented TARGET images and compute softmax.\n",
        "            logits_target = combined_logits[total_source:]\n",
        "            logits_target_w = logits_target[: tf.shape(target_w)[0]]\n",
        "            y_hat_target_w = tf.nn.softmax(logits_target_w)\n",
        "\n",
        "            # Align the target label distribution to that of the source.\n",
        "            expectation_ratio = tf.reduce_mean(y_hat_source_w) / tf.reduce_mean(\n",
        "                y_hat_target_w\n",
        "            )\n",
        "            y_tilde_target_w = tf.math.l2_normalize(\n",
        "                y_hat_target_w * expectation_ratio, 1\n",
        "            )\n",
        "\n",
        "            ## 3. Relative confidence thresholding ##\n",
        "            row_wise_max = tf.reduce_max(y_hat_source_w, axis=-1)\n",
        "            final_sum = tf.reduce_mean(row_wise_max, 0)\n",
        "            c_tau = self.tau * final_sum\n",
        "            mask = tf.reduce_max(y_tilde_target_w, axis=-1) >= c_tau\n",
        "\n",
        "            ## Compute losses (pay attention to the indexing) ##\n",
        "            source_loss = compute_loss_source(\n",
        "                source_labels,\n",
        "                final_source_logits[: tf.shape(source_w)[0]],\n",
        "                final_source_logits[tf.shape(source_w)[0] :],\n",
        "            )\n",
        "            target_loss = compute_loss_target(\n",
        "                y_tilde_target_w, logits_target[tf.shape(target_w)[0] :], mask\n",
        "            )\n",
        "\n",
        "            t = self.compute_mu()  # Compute weight for the target loss\n",
        "            total_loss = source_loss + (t * target_loss)\n",
        "            self.current_step.assign_add(\n",
        "                1\n",
        "            )  # Update current training step for the scheduler\n",
        "\n",
        "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "        self.loss_tracker.update_state(total_loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}"
      ],
      "metadata": {
        "id": "u-gLVPZS1CEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate a Wide ResNet 28-2"
      ],
      "metadata": {
        "id": "wHPEuDC01Od7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wide_basic(x, n_input_plane, n_output_plane, stride):\n",
        "    conv_params = [[3, 3, stride, \"same\"], [3, 3, (1, 1), \"same\"]]\n",
        "\n",
        "    n_bottleneck_plane = n_output_plane\n",
        "\n",
        "    # Residual block\n",
        "    for i, v in enumerate(conv_params):\n",
        "        if i == 0:\n",
        "            if n_input_plane != n_output_plane:\n",
        "                x = layers.BatchNormalization()(x)\n",
        "                x = layers.Activation(\"relu\")(x)\n",
        "                convs = x\n",
        "            else:\n",
        "                convs = layers.BatchNormalization()(x)\n",
        "                convs = layers.Activation(\"relu\")(convs)\n",
        "            convs = layers.Conv2D(\n",
        "                n_bottleneck_plane,\n",
        "                (v[0], v[1]),\n",
        "                strides=v[2],\n",
        "                padding=v[3],\n",
        "                kernel_initializer=INIT,\n",
        "                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        "                use_bias=False,\n",
        "            )(convs)\n",
        "        else:\n",
        "            convs = layers.BatchNormalization()(convs)\n",
        "            convs = layers.Activation(\"relu\")(convs)\n",
        "            convs = layers.Conv2D(\n",
        "                n_bottleneck_plane,\n",
        "                (v[0], v[1]),\n",
        "                strides=v[2],\n",
        "                padding=v[3],\n",
        "                kernel_initializer=INIT,\n",
        "                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        "                use_bias=False,\n",
        "            )(convs)\n",
        "\n",
        "    # Shortcut connection: identity function or 1x1\n",
        "    # convolutional\n",
        "    #  (depends on difference between input & output shape - this\n",
        "    #   corresponds to whether we are using the first block in\n",
        "    #   each\n",
        "    #   group; see `block_series()`).\n",
        "    if n_input_plane != n_output_plane:\n",
        "        shortcut = layers.Conv2D(\n",
        "            n_output_plane,\n",
        "            (1, 1),\n",
        "            strides=stride,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=INIT,\n",
        "            kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        "            use_bias=False,\n",
        "        )(x)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    return layers.Add()([convs, shortcut])\n",
        "\n",
        "\n",
        "# Stacking residual units on the same stage\n",
        "def block_series(x, n_input_plane, n_output_plane, count, stride):\n",
        "    x = wide_basic(x, n_input_plane, n_output_plane, stride)\n",
        "    for i in range(2, int(count + 1)):\n",
        "        x = wide_basic(x, n_output_plane, n_output_plane, stride=1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_network(image_size=32, num_classes=10):\n",
        "    n = (DEPTH - 4) / 6\n",
        "    n_stages = [16, 16 * WIDTH_MULT, 32 * WIDTH_MULT, 64 * WIDTH_MULT]\n",
        "\n",
        "    inputs = keras.Input(shape=(image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    conv1 = layers.Conv2D(\n",
        "        n_stages[0],\n",
        "        (3, 3),\n",
        "        strides=1,\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=INIT,\n",
        "        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        "        use_bias=False,\n",
        "    )(x)\n",
        "\n",
        "    ## Add wide residual blocks ##\n",
        "\n",
        "    conv2 = block_series(\n",
        "        conv1,\n",
        "        n_input_plane=n_stages[0],\n",
        "        n_output_plane=n_stages[1],\n",
        "        count=n,\n",
        "        stride=(1, 1),\n",
        "    )  # Stage 1\n",
        "\n",
        "    conv3 = block_series(\n",
        "        conv2,\n",
        "        n_input_plane=n_stages[1],\n",
        "        n_output_plane=n_stages[2],\n",
        "        count=n,\n",
        "        stride=(2, 2),\n",
        "    )  # Stage 2\n",
        "\n",
        "    conv4 = block_series(\n",
        "        conv3,\n",
        "        n_input_plane=n_stages[2],\n",
        "        n_output_plane=n_stages[3],\n",
        "        count=n,\n",
        "        stride=(2, 2),\n",
        "    )  # Stage 3\n",
        "\n",
        "    batch_norm = layers.BatchNormalization()(conv4)\n",
        "    relu = layers.Activation(\"relu\")(batch_norm)\n",
        "\n",
        "    # Classifier\n",
        "    trunk_outputs = layers.GlobalAveragePooling2D()(relu)\n",
        "    outputs = layers.Dense(\n",
        "        num_classes, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
        "    )(trunk_outputs)\n",
        "\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "ey7-D8vo1RrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrn_model = get_network()\n",
        "print(f\"Model has {wrn_model.count_params()/1e6} Million parameters.\")"
      ],
      "metadata": {
        "id": "xDlnVn1_1bJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate AdaMatch model and compil it"
      ],
      "metadata": {
        "id": "4haroA2j1b1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = keras.optimizers.schedules.CosineDecay(LEARNING_RATE, TOTAL_STEPS, 0.25)\n",
        "optimizer = keras.optimizers.Adam(reduce_lr)\n",
        "\n",
        "adamatch_trainer = AdaMatch(model=wrn_model, total_steps=TOTAL_STEPS)\n",
        "adamatch_trainer.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "98Ckp0Qb1fKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training"
      ],
      "metadata": {
        "id": "9fB7DEGs1hvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_ds = tf.data.Dataset.zip((final_source_ds, final_target_ds))\n",
        "adamatch_trainer.fit(total_ds, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "mHN3YdUR1jnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on the target and source test sets"
      ],
      "metadata": {
        "id": "LG979Qms1mwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the AdaMatch model to yield accuracy.\n",
        "adamatch_trained_model = adamatch_trainer.model\n",
        "adamatch_trained_model.compile(metrics=keras.metrics.SparseCategoricalAccuracy())\n",
        "\n",
        "# Score on the target test set.\n",
        "svhn_test = svhn_test.batch(TARGET_BATCH_SIZE).prefetch(AUTO)\n",
        "_, accuracy = adamatch_trained_model.evaluate(svhn_test)\n",
        "print(f\"Accuracy on target test set: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "4GZvlHkz1qv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function for preprocessing the source test set.\n",
        "def prepare_test_ds_source(image, label):\n",
        "    image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n",
        "    image = tf.tile(image, [1, 1, 3])\n",
        "    return image, label\n",
        "\n",
        "\n",
        "source_test_ds = tf.data.Dataset.from_tensor_slices((mnist_x_test, mnist_y_test))\n",
        "source_test_ds = (\n",
        "    source_test_ds.map(prepare_test_ds_source, num_parallel_calls=AUTO)\n",
        "    .batch(TARGET_BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Evaluation on the source test set.\n",
        "_, accuracy = adamatch_trained_model.evaluate(source_test_ds)\n",
        "print(f\"Accuracy on source test set: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "5u_IlO951s6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}